{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import altair as alt\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import sys\n",
    "from ocp_table_tpot.globals import Globals as gd\n",
    "from tpot import TPOTRegressor\n",
    "sys.path.insert(0,'..')\n",
    "from src.models.model import HistoricalMedian,XGBoost,LinearModel,RF,KNN,SVM,mase,TimeSeriesSplitImproved\n",
    "from sklearn.linear_model import ElasticNet, Lasso,  BayesianRidge, LassoLarsIC,RANSACRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor,  GradientBoostingRegressor,ExtraTreesRegressor\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import EarlyStopping,ModelCheckpoint\n",
    "from keras_tqdm import TQDMNotebookCallback\n",
    "from keras.initializers import random_uniform\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Sequential\n",
    "from keras.optimizers import Adam,SGD\n",
    "from keras.initializers import random_uniform,glorot_uniform\n",
    "from keras.layers import Dense,Dropout,BatchNormalization,Activation\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint,Callback,ReduceLROnPlateau\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import QuantileTransformer\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import RobustScaler,MinMaxScaler,PolynomialFeatures,StandardScaler\n",
    "from sklearn.base import BaseEstimator, TransformerMixin, RegressorMixin, clone\n",
    "from sklearn.model_selection import KFold, cross_val_score, train_test_split\n",
    "from skgarden.quantile import RandomForestQuantileRegressor\n",
    "from sklearn.metrics import mean_squared_error,make_scorer\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from copy import copy\n",
    "from tpot.builtins import StackingEstimator\n",
    "from lightgbm import LGBMRegressor\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "from sklearn.pipeline import make_pipeline, make_union\n",
    "from catboost import CatBoostRegressor,Pool,cv\n",
    "import umap\n",
    "\n",
    "df_tsfresh = pd.read_pickle(f'../data/processed/train_test_tsfresh_6.pkl').reset_index(level = 0)\n",
    "data_dict = pd.read_pickle(f'../data/processed/data_dict_all.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "year = 2019\n",
    "tgt = 'rougher.output.recovery'\n",
    "X = data_dict[year]['X_train_tsclean']\n",
    "y = data_dict[year]['y_train_tsclean']\n",
    "print(f'1) X shape: {X.shape},y: {y.shape}')\n",
    "\n",
    "\n",
    "X_test=  data_dict[year]['X_test_ts']\n",
    "print(f'1) Test shape: {X_test.shape}, train: {X.shape}')\n",
    "\n",
    "inds_y = y[(y[tgt] > 5) & (y[tgt] < 100)].index\n",
    "inds_common = inds_y\n",
    "X = X.loc[inds_common,]\n",
    "\n",
    "print(f'2) Test shape: {X_test.shape}, train: {X.shape}')\n",
    "\n",
    "y = y.loc[inds_common, tgt]\n",
    "\n",
    "X = X.sample(frac=0.8,random_state=123).sort_index().dropna()\n",
    "y= y[X.index]\n",
    "X_filt = X.filter(regex  =\"rougher\",axis = 1)\n",
    "X_filt.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Base models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_folds = 8\n",
    "\n",
    "lasso = make_pipeline(RobustScaler(), Lasso(alpha =0.005, random_state=1,max_iter = 2000))\n",
    "\n",
    "def cv_catboost(model,X_base=None,y=None):\n",
    "#     pool = Pool(X_base,y)\n",
    "#     #,task_type='GPU',od_type = 'Iter',od_wait= 15\n",
    "#     params['task_type'] = \"GPU\"\n",
    "#     params['od_type'] = \"Iter\"\n",
    "#     params['od_wait'] = 20\n",
    "#     params['has_time']= True\n",
    "#     params['plot']=False\n",
    "#     params['verbose_eval'] = 100\n",
    "#     params['shuffle'] = False\n",
    "#     params['fold_count'] = 5\n",
    "#     params['early_stopping_rounds'] = 15\n",
    "#     params['random_seed'] = 123\n",
    "#     scores = cv(pool,params)\n",
    "   \n",
    "    cv = KFold(n_folds, shuffle=False, random_state=42)\n",
    "    scores = []\n",
    "    preds_all_alt = np.empty_like(y)\n",
    "    preds_all_base = np.empty_like(y)\n",
    "    \n",
    "    true_all =np.empty_like(y)\n",
    "\n",
    "    for fold_n, (train_index, valid_index) in enumerate(cv.split(X)):\n",
    "    # print('Fold', fold_n, 'started at', time.ctime())\n",
    "        X_train, X_valid = X.iloc[train_index], X.iloc[valid_index]\n",
    "        y_train, y_valid = y.iloc[train_index], y.iloc[valid_index]\n",
    "        \n",
    "        \n",
    "        print(y.shape)\n",
    "        # Do the base\n",
    "        model.fit(X_train.values,y_train.values.reshape(-1,),eval_set=(X_valid.values, y_valid.values.reshape(-1,)),plot=False,verbose_eval = 300)\n",
    "        preds  = model.predict(X_valid.values)\n",
    "        preds_all_base[valid_index] = preds\n",
    "        true_all[valid_index] = y_valid\n",
    "        \n",
    "    oof_scores = mase(preds_all_base,true_all)\n",
    "    return oof_scores\n",
    "\n",
    "\n",
    "class StackingAveragedKerasModels(BaseEstimator, RegressorMixin, TransformerMixin):\n",
    "    def __init__(self, base_models, meta_model, n_folds=6):\n",
    "        self.base_models = base_models\n",
    "        self.meta_model = meta_model\n",
    "        self.n_folds = n_folds\n",
    "   \n",
    "    # We again fit the data on clones of the original models\n",
    "    def fit(self, X, y,tgt='Test',year = \"year\"):\n",
    "        self.base_models_ = [list() for x in self.base_models]\n",
    "        self.meta_model_ = clone(self.meta_model)\n",
    "        kfold = KFold(n_splits=self.n_folds, shuffle=False, random_state=156)\n",
    "        \n",
    "        \n",
    "        # Train cloned base models then create out-of-fold predictions\n",
    "        # that are needed to train the cloned meta-model\n",
    "        out_of_fold_predictions = np.zeros((X.shape[0], len(self.base_models)))\n",
    "        for i, model in enumerate(self.base_models):\n",
    "            k=0\n",
    "            for train_index, holdout_index in kfold.split(X, y):\n",
    "                instance = clone(model)\n",
    "                self.base_models_[i].append(instance)\n",
    "                \n",
    "                        # Set up callbacks\n",
    "                model_id = np.random.randint(low = 0,high = 10000,size = 1)\n",
    "                fpath = f'./keras-ch/{k}_{tgt}_{i}_{year}.h5'\n",
    "                callbacks = [EarlyStopping(monitor='val_loss', patience=20),TQDMNotebookCallback(leave_inner=False),#ReduceLROnPlateau(monitor='val_loss', patience=5, verbose=1),\n",
    "                     ModelCheckpoint(filepath=fpath, monitor='val_loss', save_best_only=True)]\n",
    "                fit_params = {\"epochs\":150,\n",
    "                     \"verbose\":0,\n",
    "                     \"batch_size\":64,\n",
    "                     \"callbacks\":callbacks}\n",
    "                X_valid = np.array(X)[holdout_index,:]\n",
    "                y_valid = np.array(y)[holdout_index]\n",
    "                \n",
    "                X_train = np.array(X)[train_index,:]\n",
    "                y_train = np.array(y)[train_index]\n",
    "                \n",
    "                fit_params[\"validation_data\"] = (X_valid, y_valid)\n",
    "                print(X_train.shape)\n",
    "                print(y_train.shape)\n",
    "                print(X_valid.shape)\n",
    "                print(y_valid.shape)\n",
    "                \n",
    "                \n",
    "                instance.fit(X_train, y_train,**fit_params)\n",
    "                instance.model.load_weights(fpath)\n",
    "                \n",
    "                y_pred = instance.predict(np.array(X)[holdout_index])\n",
    "                out_of_fold_predictions[holdout_index, i] = y_pred\n",
    "                k=k+1\n",
    "        # Now train the cloned  meta-model using the out-of-fold predictions as new feature\n",
    "        self.meta_model_.fit(out_of_fold_predictions, y)\n",
    "        return self\n",
    "   \n",
    "    #Do the predictions of all base models on the test data and use the averaged predictions as \n",
    "    #meta-features for the final prediction which is done by the meta-model\n",
    "    def predict(self, X):\n",
    "        meta_features = np.column_stack([\n",
    "            np.column_stack([model.predict(X) for model in base_models]).mean(axis=1)\n",
    "            for base_models in self.base_models_ ])\n",
    "        return self.meta_model_.predict(meta_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pull  KERAS models as basic models, do a stacking prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "def huber_loss(y_true, y_pred, clip_delta=1.0):\n",
    "  error = y_true - y_pred\n",
    "  cond  = tf.keras.backend.abs(error) < clip_delta\n",
    "\n",
    "  squared_loss = 0.5 * tf.keras.backend.square(error)\n",
    "  linear_loss  = clip_delta * (tf.keras.backend.abs(error) - 0.5 * clip_delta)\n",
    "\n",
    "  return tf.where(cond, squared_loss, linear_loss)\n",
    "\n",
    "def tilted_loss( y, f,q = 0.55):\n",
    "    e = (y - f)\n",
    "    return K.mean(K.maximum(q * e, (q - 1) * e),\n",
    "                              axis=-1)\n",
    "\n",
    "def huber_loss_mean(y_true, y_pred, clip_delta=1.0):\n",
    "  return tf.keras.backend.mean(huber_loss(y_true, y_pred, clip_delta))\n",
    "\n",
    "\n",
    "def create_a(optimizer=Adam(lr=0.01),\n",
    "                 kernel_initializer=random_uniform(), \n",
    "                 dropout=0.1,input_shape = (1,175)):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(1024,input_dim = input_shape,kernel_initializer=kernel_initializer))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(dropout))\n",
    "    \n",
    "    model.add(Dense(8,kernel_initializer=kernel_initializer))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(dropout))\n",
    "    \n",
    "    model.add(Dense(1,kernel_initializer=kernel_initializer))\n",
    "    model.compile(loss='mae',optimizer=optimizer, metrics=['mae'])\n",
    "\n",
    "    return model\n",
    "def create_b(optimizer=Adam(lr=0.01),\n",
    "                 kernel_initializer=random_uniform(), \n",
    "                 dropout=0.1,input_shape = (1,175)):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(1024,input_dim = input_shape,kernel_initializer=kernel_initializer))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(dropout))\n",
    "    \n",
    "    model.add(Dense(8,kernel_initializer=kernel_initializer))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(dropout))\n",
    "    \n",
    "    model.add(Dense(1,kernel_initializer=kernel_initializer))\n",
    "    model.compile(loss='mae',optimizer=optimizer, metrics=['mae'])\n",
    "\n",
    "    return model\n",
    "def create_c(optimizer=Adam(lr=0.01),\n",
    "                 kernel_initializer=random_uniform(), \n",
    "                 dropout=0.1,input_shape = (1,175)):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(96,input_dim = input_shape,kernel_initializer=kernel_initializer))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(dropout))\n",
    "    \n",
    "    model.add(Dense(16,kernel_initializer=kernel_initializer))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(dropout))\n",
    "    \n",
    "    model.add(Dense(1,kernel_initializer=kernel_initializer))\n",
    "    model.compile(loss='mae',optimizer=optimizer, metrics=['mae'])\n",
    "\n",
    "    return model\n",
    "def create_d(optimizer=Adam(lr=0.01),\n",
    "                 kernel_initializer=random_uniform(), \n",
    "                 dropout=0.1,input_shape = (1,175)):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(1024,input_dim = input_shape,kernel_initializer=kernel_initializer))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(dropout))\n",
    "    \n",
    "    model.add(Dense(8,kernel_initializer=kernel_initializer))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(dropout))\n",
    "    \n",
    "    model.add(Dense(1,kernel_initializer=kernel_initializer))\n",
    "    model.compile(loss='mae',optimizer=optimizer, metrics=['mae'])\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "mod_a = KerasRegressor(build_fn=create_a,verbose=0)\n",
    "mod_b = KerasRegressor(build_fn=create_b,verbose=0)\n",
    "mod_c = KerasRegressor(build_fn=create_c,verbose=0)\n",
    "mod_d = KerasRegressor(build_fn=create_d,verbose=0)\n",
    "\n",
    "\n",
    "stacked_averaged_models = StackingAveragedKerasModels(base_models = (mod_a,mod_b,mod_c,mod_d),\n",
    "                                                 meta_model = lasso)\n",
    "#,lgb_a,lgb_b,lgb_c,lgb_d),\n",
    "#ax,score,oof_scores  = rmsle_cv_gen_compare(stacked_averaged_models,plot=True,X_base=X,y=y,X_alt=X_filt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(\"Stacking Averaged models score: {:.4f} ({:.4f})\".format(score.mean(), score.std()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final predictions:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import IsolationForest\n",
    "irf = IsolationForest(verbose =1,contamination='auto',behaviour='new')\n",
    "\n",
    "def clean_target(y,wupper = int(24*10*2.5),wmean =int(24*8/0.4),wlower = int(24*4*2.5),wstd = int(24*30*2.5),upper_std=False ):\n",
    "    import altair as alt\n",
    "    alt.data_transformers.enable('default', max_rows=None)\n",
    "\n",
    "    tmp = pd.DataFrame(y)\n",
    "    tmp.columns=['Tgt']\n",
    "    tmp.reset_index(inplace=True)\n",
    "\n",
    "    tmp['mean'] = tmp['Tgt'].rolling(window = wmean,center=True).median().fillna(method='bfill')\n",
    "    tmp['std'] = tmp['Tgt'].rolling(window = wstd,center=True).std().fillna(method='bfill')\n",
    "    #tmp['upper'] = (tmp['mean']+ 1.5 * tmp['std']).fillna(method='bfill').fillna(method='ffill')\n",
    "    tmp['lower'] = (tmp['mean']- 1.7 * tmp['std']).fillna(method='bfill').fillna(method='ffill')\n",
    "    #tmp['lower'] = tmp['Tgt'].rolling(window = wlower,center=True).quantile(0.04).fillna(method='bfill')\n",
    "    tmp['upper'] = tmp['Tgt'].rolling(window = wupper).quantile(0.98).fillna(method='bfill')\n",
    "    if upper_std:\n",
    "        tmp['upper'] = (tmp['mean']+ 1.7 * tmp['std']).fillna(method='bfill').fillna(method='ffill')\n",
    "\n",
    "\n",
    "    tmp_melt = tmp.melt(id_vars = ['date'])\n",
    "    y = y.loc[tmp[(tmp.Tgt > tmp.lower) & (tmp.Tgt < tmp.upper)].date,]\n",
    "    \n",
    "    ch = alt.Chart(width=1000,data = tmp_melt).mark_point(filled=True,size= 5).encode(\n",
    "                x='date',\n",
    "                y='value:Q',\n",
    "                color='variable:N',\n",
    "                #shape = 'variable',\n",
    "                opacity=alt.value(0.6))\n",
    "    \n",
    "    \n",
    "    return y,tmp_melt,ch\n",
    "\n",
    "\n",
    "preds_all = []\n",
    "model = stacked_averaged_models\n",
    "\n",
    "scaler = make_pipeline(QuantileTransformer(output_distribution='normal'),StandardScaler(),PCA(whiten=True))\n",
    "\n",
    "for year in [2016,2017]:\n",
    "    print(year)\n",
    "    year_train = 2019\n",
    "    data_dict = pd.read_pickle(f'../data/processed/data_dict_all.pkl')\n",
    "    X_test = data_dict[year]['X_test_ts'].copy().filter(regex  =\"rougher\",axis = 1)\n",
    "    \n",
    "    tgt = \"rougher.output.recovery\"\n",
    "    \n",
    "    X = data_dict[year_train]['X_train_tsclean'].copy().tz_convert(None)\n",
    "    print(f'1a) Test shape: {X_test.shape}, train: {X.shape}')\n",
    "    y = data_dict[year_train]['y_train_tsclean'][tgt].copy().dropna().tz_convert(None)\n",
    "    y = y[(y>45) & (y <99)]\n",
    "    #Fit Isolation RF\n",
    "    #irf.fit(X)\n",
    "    #irf_preds = irf.predict(X)\n",
    "   # X= X.loc[irf_preds > 0]\n",
    "    print(f'1b) Test shape: {X_test.shape}, train: {X.shape} after FILT')\n",
    "    \n",
    "    #y,tmp_melt,chrr = clean_target(y,upper_std=True)\n",
    "   # print(ch)\n",
    "\n",
    "    inds = X.index.intersection(y.index)\n",
    "    X = X.loc[inds].filter(regex  =\"rougher\",axis = 1)\n",
    "    y = y.loc[inds]\n",
    "    print(f'1c) Test shape: {X_test.shape}, train: {X.shape} after intersect with Y')\n",
    "    scaler.fit(X)\n",
    "    Xsc = scaler.transform(X)\n",
    "    \n",
    "    mod_a = KerasRegressor(build_fn=create_a,input_shape = Xsc.shape[1],verbose=0)\n",
    "    mod_b = KerasRegressor(build_fn=create_b,input_shape = Xsc.shape[1],verbose=0)\n",
    "    mod_c = KerasRegressor(build_fn=create_c,input_shape = Xsc.shape[1],verbose=0)\n",
    "    mod_d = KerasRegressor(build_fn=create_d,input_shape = Xsc.shape[1],verbose=0)\n",
    "    model = StackingAveragedKerasModels(base_models = (mod_a,mod_b,mod_c,mod_d),\n",
    "                                                     meta_model = lasso)\n",
    "    \n",
    "    model.fit(Xsc, y,tgt=tgt,year = year)\n",
    "    Xtestsc = scaler.transform(X_test)\n",
    "    \n",
    "    ypred_r = model.predict(Xtestsc)\n",
    "    preds_r = pd.DataFrame(data = {'date':X_test.index, tgt:ypred_r}).set_index('date')\n",
    "    \n",
    "    tgt = \"final.output.recovery\"\n",
    "    \n",
    "    X = data_dict[year_train]['X_train_tsclean'].copy().tz_convert(None)\n",
    "    X_test = data_dict[year]['X_test_ts'].copy()\n",
    "    y = data_dict[year_train]['y_train_tsclean'][tgt].copy().dropna().tz_convert(None)\n",
    "    y = y[(y>35) & (y <97)]\n",
    "    print(f'2a) Test shape: {X_test.shape}, train: {X.shape}')\n",
    "    # Data cleaning:\n",
    "   \n",
    "   # y,tmp_melt,chf = clean_target(y,wupper =24*10*5,wstd=24*50,upper_std=True)\n",
    "   # print(ch)\n",
    "   # irf.fit(X)\n",
    "   # irf_preds = irf.predict(X)\n",
    "   # X= X.loc[irf_preds > 0]\n",
    "    print(f'2b) Test shape: {X_test.shape}, train: {X.shape} after FILT')\n",
    "    \n",
    "    inds = X.index.intersection(y.index)\n",
    "    X = X.loc[inds]\n",
    "    y = y.loc[inds]\n",
    "    print(f'2c) Test shape: {X_test.shape}, train: {X.shape} after intersect with Y')\n",
    "    scaler.fit(X)\n",
    "    Xsc = scaler.transform(X)\n",
    "    \n",
    "    Xtestsc = scaler.transform(X_test)\n",
    "    mod_a = KerasRegressor(build_fn=create_a,input_shape = Xsc.shape[1],verbose=0)\n",
    "    mod_b = KerasRegressor(build_fn=create_b,input_shape = Xsc.shape[1],verbose=0)\n",
    "    mod_c = KerasRegressor(build_fn=create_c,input_shape = Xsc.shape[1],verbose=0)\n",
    "    mod_d = KerasRegressor(build_fn=create_d,input_shape = Xsc.shape[1],verbose=0)\n",
    "    model = StackingAveragedKerasModels(base_models = (mod_a,mod_b,mod_c,mod_d),\n",
    "                                                     meta_model = lasso)\n",
    "    \n",
    "    model.fit(Xsc, y,tgt=tgt,year = year)\n",
    "    ypred_f = model.predict(Xtestsc)\n",
    "    preds_f = pd.DataFrame(data = {'date':X_test.index, tgt:ypred_f}).set_index('date')\n",
    "\n",
    "    preds_all.append(preds_r.join(preds_f))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stacked_preds_sub = pd.concat(preds_all)\n",
    "stacked_preds_sub = stacked_preds_sub.reset_index()\n",
    "stacked_preds_sub['date'] = stacked_preds_sub['date'].dt.strftime('%Y-%m-%dT%H:%M:%SZ')\n",
    "stacked_preds_sub.set_index('date',inplace=True)\n",
    "stacked_preds_sub.drop_duplicates(inplace=True)\n",
    "stacked_preds_sub.to_csv('../results/stacked_sub_lgb_lasso_base_alldata_tsclean_keras.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stacked_preds_sub.plot(x='rougher.output.recovery',y='final.output.recovery',xlim=(60,100),ylim=(30,100),kind='scatter')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ypred_r"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Average Keras Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preload the models according to the architecture\n",
    "tgt = ['rougher.output.recovery','final.output.recovery']\n",
    "year = [2016,2017]\n",
    "models_final = {}\n",
    "models_rougher = {}\n",
    "# 332 , 175\n",
    "\n",
    "list_models_rougher = [create_a(input_shape = 175),create_b(input_shape = 175),create_c(input_shape = 175),create_d(input_shape = 175)]\n",
    "list_models_final = [create_a(input_shape = 332),create_b(input_shape = 332),create_c(input_shape = 332),create_d(input_shape = 332)]\n",
    "\n",
    "for i in range(4):\n",
    "    for fold in range(6):\n",
    "        for y in year:\n",
    "            models_rougher[f'{fold}-{y}-{i}'] = list_models_rougher[i]\n",
    "            fpathr = f'./keras-ch/{fold}_{tgt[0]}_{i}_{y}.h5'\n",
    "            models_rougher[f'{fold}-{y}-{i}'].model.load_weights(fpathr)\n",
    "    \n",
    "            #5_final.output.recovery_1_2017\n",
    "            models_final[f'{fold}-{y}-{i}'] = list_models_final[i]\n",
    "            fpathf = f'./keras-ch/{fold}_{tgt[1]}_{i}_{y}.h5'\n",
    "            models_final[f'{fold}-{y}-{i}'].model.load_weights(fpathf)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "year_train = 2019\n",
    "data_dict = pd.read_pickle(f'../data/processed/data_dict_all.pkl')\n",
    "X_test = data_dict[2019]['X_test_ts'].copy().filter(regex  =\"rougher\",axis = 1)\n",
    "tgt = \"rougher.output.recovery\"\n",
    "X = data_dict[year_train]['X_train_tsclean'].copy().tz_convert(None)\n",
    "print(f'1a) Test shape: {X_test.shape}, train: {X.shape}')\n",
    "y = data_dict[year_train]['y_train_tsclean'][tgt].copy().dropna().tz_convert(None)\n",
    "y = y[(y>45) & (y <99)]\n",
    "print(f'1b) Test shape: {X_test.shape}, train: {X.shape} after FILT')\n",
    "inds = X.index.intersection(y.index)\n",
    "X = X.loc[inds].filter(regex  =\"rougher\",axis = 1)\n",
    "y = y.loc[inds]\n",
    "scaler.fit(X)\n",
    "Xsc = scaler.transform(X)\n",
    "Xtestsc_r = scaler.transform(X_test)\n",
    "\n",
    "# ypred_r = model.predict(Xtestsc)\n",
    "# preds_r = pd.DataFrame(data = {'date':X_test.index, tgt:ypred_r}).set_index('date')\n",
    "    \n",
    "tgt = \"final.output.recovery\"\n",
    "X = data_dict[year_train]['X_train_tsclean'].copy().tz_convert(None)\n",
    "X_test = data_dict[2019]['X_test_ts'].copy()\n",
    "y = data_dict[year_train]['y_train_tsclean'][tgt].copy().dropna().tz_convert(None)\n",
    "y = y[(y>35) & (y <97)]\n",
    "print(f'2a) Test shape: {X_test.shape}, train: {X.shape}')\n",
    "scaler.fit(X)\n",
    "Xsc = scaler.transform(X)\n",
    "Xtestsc_f = scaler.transform(X_test)\n",
    "\n",
    "# ypred_f = model.predict(Xtestsc)\n",
    "# preds_f = pd.DataFrame(data = {'date':X_test.index, tgt:ypred_f}).set_index('date')\n",
    "\n",
    "# preds_all.append(preds_r.join(preds_f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pr_all = np.zeros((Xtestsc_f.shape[0],2))\n",
    "k=0\n",
    "for i in range(4):\n",
    "    for fold in range(6):\n",
    "        for y in year:\n",
    "            k+=1\n",
    "            pr_all[:,0]+= models_rougher[f'{fold}-{y}-{i}'].predict(Xtestsc_r).reshape((-1,))\n",
    "            pr_all[:,1]+= models_final[f'{fold}-{y}-{i}'].predict(Xtestsc_f).reshape((-1,))\n",
    "pr_all = pr_all / k\n",
    "preds_all = pd.DataFrame(data = pr_all,columns = ['rougher.output.recovery','final.output.recovery'],index = X_test.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_all.plot(figsize = (20,6),style = ['o','o'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
