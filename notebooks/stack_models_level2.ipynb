{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import altair as alt\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import sys\n",
    "from ocp_table_tpot.globals import Globals as gd\n",
    "from tpot import TPOTRegressor\n",
    "sys.path.insert(0,'..')\n",
    "\n",
    "from sklearn.pipeline import make_pipeline, make_union\n",
    "from sklearn.preprocessing import RobustScaler,MinMaxScaler,PolynomialFeatures,QuantileTransformer,Normalizer\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.base import BaseEstimator, TransformerMixin, RegressorMixin, clone\n",
    "from sklearn.model_selection import KFold, cross_val_score, train_test_split\n",
    "\n",
    "from sklearn.metrics import mean_squared_error,make_scorer\n",
    "from copy import copy\n",
    "from tpot.builtins import StackingEstimator\n",
    "\n",
    "\n",
    "from src.models.model import mase,TimeSeriesSplitImproved\n",
    "from sklearn.linear_model import ElasticNet, Lasso,  BayesianRidge, LassoLarsIC,RANSACRegressor,Ridge\n",
    "from sklearn.ensemble import RandomForestRegressor, ExtraTreesRegressor\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from skgarden.quantile import RandomForestQuantileRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "import umap\n",
    "\n",
    "\n",
    "df_tsfresh = pd.read_pickle(f'../data/processed/train_test_tsfresh_6.pkl').reset_index(level = 0)\n",
    "data_dict = pd.read_pickle(f'../data/processed/data_dict_all.pkl')\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "year = 2019\n",
    "tgt = 'rougher.output.recovery'\n",
    "\n",
    "X = data_dict[year]['X_train_tsclean']\n",
    "print(X.shape)\n",
    "y = data_dict[year]['y_train']\n",
    "X_test = data_dict[year]['X_test_ts']\n",
    "mask = data_dict[year]['mask']\n",
    "exclude_pts = data_dict[year]['excl'].set_index('date').tz_localize('UTC')\n",
    "#mask_na_two_row=y[y[tgt].isna()].index.union(y[y[tgt].isna()].index + pd.Timedelta('1 hour')).union(y[y[tgt].isna()].index + pd.Timedelta('2 hour'))\n",
    "inds = mask.index.difference(exclude_pts.index)\n",
    "\n",
    "X=X.loc[inds,:]\n",
    "y=y.loc[inds,:]\n",
    "mask=mask[inds]\n",
    "print(X.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'1) X shape: {X.shape},y: {y.shape}')\n",
    "X = X[mask]\n",
    "y = y[mask][tgt]\n",
    "print(f'2) Train shape: {X.shape}')\n",
    "X_filt = X.filter(regex  =\"rougher|hour|dayw\",axis = 1)\n",
    "X = X_filt\n",
    "train_df = pd.concat([X,y],axis= 1)\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fit Isolation RF\n",
    "from sklearn.ensemble import IsolationForest\n",
    "irf = IsolationForest(verbose =1,contamination='auto',behaviour='new')\n",
    "\n",
    "irf.fit(train_df[X.columns])\n",
    "\n",
    "irf_preds = irf.predict(train_df[X.columns])\n",
    "\n",
    "(irf_preds > 0).sum()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a function to fit a base model on `K-1` folds, predict on `1` fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function takes one model and fit it to the train and test data\n",
    "# It returns the model MASE, CV prediction, and test prediction\n",
    "def base_fit(model, folds, features, target, trainData, testData):\n",
    "    # Initialize empty lists and matrix to store data\n",
    "    model_mase = []\n",
    "    model_val_predictions = np.empty((trainData.shape[0], 1))\n",
    "    k=0\n",
    "    # Loop through the index in KFolds\n",
    "    model_test_predictions=np.zeros((testData.shape[0],))\n",
    "    model_val_true=np.zeros((trainData.shape[0],1))\n",
    "\n",
    "    for train_index, val_index in folds.split(trainData):\n",
    "        k=k+1\n",
    "        # Split the train data into train and validation data\n",
    "        train, validation = trainData.iloc[train_index], trainData.iloc[val_index]\n",
    "        # Get the features and target\n",
    "        train_features, train_target = train[features], train[target]\n",
    "        validation_features, validation_target = validation[features], validation[target]\n",
    "        \n",
    "        # Fit the base model to the train data and make prediciton for validation data\n",
    "        if (model.__class__ == xgb.sklearn.XGBRegressor) | (model.__class__ == lgb.sklearn.LGBMRegressor): \n",
    "            print('Fitting a boost model with limited tree rounds')\n",
    "            evalset = [(validation_features,np.ravel(validation_target))]\n",
    "            model.fit(train_features, np.ravel(train_target),eval_set =evalset,early_stopping_rounds = 20,verbose = False)\n",
    "        else:\n",
    "            model.fit(train_features, train_target.values)\n",
    "        \n",
    "        if (model.__class__ == xgb.sklearn.XGBRegressor):\n",
    "            print(model.best_ntree_limit)\n",
    "            print('Using xgboost with limited tree rounds')\n",
    "            validation_predictions = model.predict(validation_features,ntree_limit = model.best_ntree_limit)\n",
    "\n",
    "        elif (model.__class__ == lgb.sklearn.LGBMRegressor):\n",
    "            print(model.best_iteration_)\n",
    "            print('Using lgbmboost with limited tree rounds')\n",
    "            validation_predictions = model.predict(validation_features,num_iteration = model.best_iteration_)\n",
    "        else:\n",
    "            print('Using generic predict')\n",
    "            validation_predictions = model.predict(validation_features)\n",
    "        \n",
    "        \n",
    "        # Calculate and store the MASE for validation data\n",
    "        print(mase(validation_predictions,validation_target))\n",
    "        #model_mase.append(mase(validation_predictions,validation_target))\n",
    "        \n",
    "        # Save the validation prediction for level 1 model training\n",
    "        model_val_predictions[val_index, 0] = validation_predictions.reshape(validation.shape[0])\n",
    "        model_val_true[val_index,0] = validation_target.values\n",
    "        model_test_predictions += model.predict(testData[features])    \n",
    "        \n",
    "    model_test_predictions = model_test_predictions/k\n",
    "    # Fit the base model to the whole training data\n",
    "    #model.fit(trainData[features], np.ravel(trainData[target]))\n",
    "    # Get base model prediction for the test data\n",
    "    #model_test_predictions = model.predict(testData[features])\n",
    "    # Calculate and store the MASE for validation data\n",
    "        \n",
    "    #model_val_predictions = model_val_predictions\n",
    "    model_mase.append(mase(model_val_predictions,model_val_true))\n",
    "    \n",
    "    return(model_mase, model_val_predictions, model_test_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a function to fit a dictionary of models, and get their OOF predictions from the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function that takes a dictionary of models and fits it to the data using baseFit\n",
    "# The results of the models are then aggregated and returned for level 1 model training\n",
    "def stacks(level0_models, folds, features, target, trainData, testData):\n",
    "    num_models = len(level0_models.keys()) #Number of models\n",
    "    \n",
    "    # Initialize empty lists and matrix\n",
    "    level0_trainFeatures = np.empty((trainData.shape[0], num_models))\n",
    "    level0_testFeatures = np.empty((testData.shape[0], num_models))\n",
    "    \n",
    "    # Loop through the models\n",
    "    for i, key in enumerate(level0_models.keys()):\n",
    "        print('Fitting %s -----------------------' % (key))\n",
    "        model_mase, val_predictions, test_predictions = base_fit(level0_models[key], folds, features, target, trainData, testData)\n",
    "        \n",
    "        # Print the average MASE for the model\n",
    "        print('%s average MASE: %s' % (key, np.mean(model_mase)))\n",
    "        print('\\n')\n",
    "        \n",
    "        # Aggregate the base model validation and test data predictions\n",
    "        level0_trainFeatures[:, i] = val_predictions.reshape(trainData.shape[0])\n",
    "        level0_testFeatures[:, i] = test_predictions.reshape(testData.shape[0])\n",
    "        \n",
    "    return(level0_trainFeatures, level0_testFeatures)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a function that trains a dictionary of stackers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function that takes a dictionary of classifiers and train them on base model predictions\n",
    "def stackerTraining(stacker, folds, level0_trainFeatures, level0_testFeatures, trainData,target = None):\n",
    "    for k in stacker.keys():\n",
    "        print('Training stacker %s' % (k))\n",
    "        stacker_model = stacker[k]\n",
    "        stacker_mase = []\n",
    "        y_pred = np.zeros_like(trainData[target].values)\n",
    "        y_true =  np.zeros_like(trainData[target].values)\n",
    "        for t, v in folds.split(X, y):\n",
    "            train, validation = level0_trainFeatures[t,:], level0_trainFeatures[v,:]\n",
    "            # Get the features and target\n",
    "            train_features, train_target = train, trainData.iloc[t][target]\n",
    "            validation_features, validation_target = validation, trainData.iloc[v][target]\n",
    "            \n",
    "            \n",
    "            if (stacker_model.__class__ == xgb.sklearn.XGBRegressor) | (stacker_model.__class__ == lgb.sklearn.LGBMRegressor): \n",
    "                print('Fitting a boost model with limited tree rounds')\n",
    "                evalset = [(validation_features,np.ravel(validation_target))]\n",
    "                stacker_model.fit(train_features, np.ravel(train_target),eval_set =evalset,early_stopping_rounds = 20,verbose = False)\n",
    "                print(stacker_model.best_iteration_)\n",
    "            else:\n",
    "                stacker_model.fit(level0_trainFeatures[t,:], train_target)\n",
    "                \n",
    "            y_pred[v] = stacker_model.predict(level0_trainFeatures[v])\n",
    "            y_true[v] = trainData.iloc[v][target].values\n",
    "        \n",
    "        stacker_mase =mase(y_pred,y_true)\n",
    "        average_mase = mase(level0_trainFeatures.mean(axis=1),y_true)\n",
    "        print('%s Stacker MASE: %s' % (k, stacker_mase))\n",
    "        print('%s Averaging MASE: %s' % (k, average_mase))\n",
    "\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the K fold indexes\n",
    "n_folds = 5\n",
    "kf = KFold(n_splits=n_folds, shuffle=False, random_state=156)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up the dictionaries of level 0 and level 1 models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A dictionary of base models\n",
    "scaler = make_pipeline(QuantileTransformer(output_distribution='normal'),PCA(whiten=True))\n",
    "level0_models = {}\n",
    "# level0_models['Lasso'] = make_pipeline(scaler, Lasso(alpha =0.005, random_state=1,max_iter = 2000))\n",
    "# level0_models['ElasticNet'] = make_pipeline(scaler,ElasticNet(alpha = 0.001))\n",
    "# level0_models['XGB_rougher_base_a'] = xgb.XGBRegressor(learning_rate=0.05,\n",
    "#                             n_estimators=400,**{'max_depth': 7, 'gamma': '44.954', 'colsample_bytree': '0.395', 'subsample': '0.993', 'min_child_weight': '133.132'},\n",
    "#                             silent=1,\n",
    "#                              random_state =48, nthread = -1)\n",
    "\n",
    "# level0_models['XGB_rougher_base_b'] =xgb.XGBRegressor(learning_rate=0.05,\n",
    "#                             n_estimators=400,**{'max_depth': 7, 'gamma': '44.954', 'colsample_bytree': '0.395', 'subsample': '0.993', 'min_child_weight': '133.132'},\n",
    "#                             silent=1,\n",
    "#                              random_state =38, nthread = -1)\n",
    "\n",
    "# # level0_models['XGB_rougher_base_c'] = xgb.XGBRegressor(learning_rate=0.05,\n",
    "# #                             n_estimators=400,**{'max_depth': 4, 'gamma': '5.102', 'colsample_bytree': '0.344', 'subsample': '0.884', 'min_child_weight': '9.622'},\n",
    "# #                             silent=1,\n",
    "# #                              random_state =82, nthread = -1)\n",
    "\n",
    "# # level0_models['XGB_rougher_base_d'] = xgb.XGBRegressor(learning_rate=0.05,\n",
    "# #                             n_estimators=400,**{'max_depth': 5, 'gamma': '3.906', 'colsample_bytree': '0.401', 'subsample': '0.760', 'min_child_weight': '9.563'},\n",
    "# #                             silent=1,\n",
    "# #                              random_state =11, nthread = -1)\n",
    "\n",
    "\n",
    "level0_models['LGBM_rougher_base_a'] = lgb.LGBMRegressor(objective='mae',\n",
    "                              learning_rate=0.05, n_estimators=500,random_state=91,\n",
    "                              **{'max_depth': 5, 'num_leaves': 100, 'feature_fraction': '0.363', 'bagging_fraction': '0.262'})\n",
    "\n",
    "level0_models['LGBM_rougher_base_b'] =lgb.LGBMRegressor(objective='mae',\n",
    "                              learning_rate=0.05, n_estimators=500,random_state=92,\n",
    "                              **{'max_depth': 4, 'num_leaves': 110, 'feature_fraction': '0.448', 'bagging_fraction': '0.445'})\n",
    "level0_models['LGBM_rougher_base_c'] =lgb.LGBMRegressor(objective='mae',\n",
    "                              learning_rate=0.05, n_estimators=500,random_state=93,\n",
    "                              **{'max_depth': 4, 'num_leaves': 155, 'feature_fraction': '0.449', 'bagging_fraction': '0.598'})\n",
    "level0_models['LGBM_rougher_base_d'] =lgb.LGBMRegressor(objective='mae',\n",
    "                              learning_rate=0.05, n_estimators=500,random_state=94,\n",
    "                             **{'max_depth': 5, 'num_leaves': 210, 'feature_fraction': '0.472', 'bagging_fraction': '0.682'})\n",
    "level0_models['LGBM_rougher_base_e']= lgb.LGBMRegressor(objective='mae',\n",
    "                              learning_rate=0.05, n_estimators=500,random_state=7,\n",
    "                              **{'max_depth': 5, 'num_leaves': 200, 'feature_fraction': '0.45', 'bagging_fraction': '0.72'})\n",
    "obj = 'mae'\n",
    "level0_models['LGBM_rougher_base_f']= lgb.LGBMRegressor(objective=obj,\n",
    "                              learning_rate=0.07, n_estimators=500,random_state=8,\n",
    "                              **{'max_depth': 4, 'num_leaves': 63, 'feature_fraction': '0.879', 'bagging_fraction': '0.727'})\n",
    "level0_models['LGBM_rougher_base_g']= lgb.LGBMRegressor(objective=obj,\n",
    "                              learning_rate=0.07, n_estimators=500,random_state=9,\n",
    "                              **{'max_depth': 5, 'num_leaves': 65, 'feature_fraction': '0.879', 'bagging_fraction': '0.727'})\n",
    "level0_models['LGBM_rougher_base_h']= lgb.LGBMRegressor(objective=obj,\n",
    "                              learning_rate=0.07, n_estimators=500,random_state=10,\n",
    "                              **{'max_depth': 4, 'num_leaves': 60, 'feature_fraction': '0.797', 'bagging_fraction': '0.982'})\n",
    "level0_models['LGBM_rougher_base_i'] = lgb.LGBMRegressor(objective=obj,\n",
    "                              learning_rate=0.07, n_estimators=500,random_state=12,\n",
    "                              **{'max_depth': 5, 'num_leaves': 60, 'feature_fraction': '0.8', 'bagging_fraction': '0.92'})\n",
    "\n",
    "# level0_models['KNN_rougher_a'] = make_pipeline(scaler,KNeighborsRegressor(n_jobs = -1,**{'n_neighbors': 254, 'weights': 'distance', 'leaf_size': 16}))\n",
    "# level0_models['KNN_rougher_b'] = make_pipeline(scaler,KNeighborsRegressor(n_jobs = -1,**{'n_neighbors': 50, 'weights': 'distance', 'leaf_size': 18}))\n",
    "# level0_models['KNN_rougher_c'] = make_pipeline(scaler,KNeighborsRegressor(n_jobs = -1,**{'n_neighbors': 15, 'weights': 'distance', 'leaf_size': 30.0}))\n",
    "# level0_models['KNN_rougher_d'] = make_pipeline(scaler,KNeighborsRegressor(n_jobs = -1,**{'n_neighbors': 5, 'weights': 'uniform', 'leaf_size': 24.0}))\n",
    "# level0_models['KNN_rougher_b_bray'] = make_pipeline(scaler,KNeighborsRegressor(n_jobs = -1,**{'n_neighbors': 50, 'weights': 'distance','metric':'braycurtis', 'leaf_size': 18}))\n",
    "# level0_models['KNN_rougher_c_bray'] = make_pipeline(scaler,KNeighborsRegressor(n_jobs = -1,**{'n_neighbors': 15, 'weights': 'distance', 'leaf_size': 30.0,'metric':'braycurtis'}))\n",
    "# level0_models['KNN_rougher_d_bray'] = make_pipeline(scaler,KNeighborsRegressor(n_jobs = -1,**{'n_neighbors': 5, 'weights': 'uniform', 'leaf_size': 24.0,'metric':'braycurtis'}))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Train all the base models in the dictionary\n",
    "level0_trainFeatures_rougher, level0_testFeatures_rougher = stacks(level0_models, kf, X.columns, tgt, train_df, X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A dictionary of level 1 model to train on base model predictions\n",
    "stacker = {\n",
    "            'Enet': ElasticNet(alpha = 0.001),\n",
    "#           'BR':BayesianRidge(n_iter = int(3e3)),\n",
    "#           'Ridge1':Ridge(),\n",
    "            'Ridge50':Ridge(alpha=60),\n",
    "           'Ridge1e2':Ridge(alpha=1e2),\n",
    "           'Ridge5e2':Ridge(alpha=5e2),\n",
    "           'Ridge1e3':Ridge(alpha=1e3),\n",
    "           'Ridge5e3':Ridge(alpha=5e3),\n",
    "          'Lasso': Lasso(alpha =0.005, random_state=1,max_iter = 2000),\n",
    "      'positive_Lasso': Lasso(alpha=0.0001,precompute=True,max_iter=1000, positive=True, random_state=9999, selection='random'),\n",
    "          'LGBM': lgb.LGBMRegressor(objective='mae',\n",
    "                              learning_rate=0.07, n_estimators=500,random_state=38,\n",
    "                              **{'max_depth': 2, 'num_leaves': 8, 'feature_fraction': '0.8', 'bagging_fraction': '0.8'})}\n",
    "\n",
    "stackerTraining(stacker, kf, level0_trainFeatures_rougher, level0_testFeatures_rougher, train_df,target = tgt)\n",
    "level1_model = stacker['LGBM'].fit(level0_trainFeatures_rougher,train_df[tgt])\n",
    "level1_test_pred_rougher = level1_model.predict(level0_testFeatures_rougher)\n",
    "rougher_meta_train = level0_trainFeatures_rougher\n",
    "rougher_meta_test = level0_testFeatures_rougher\n",
    "\n",
    "level1_test_pred_rougher.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's try to figure out the stacking Level 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def solve_pg(A, b, momentum=0.9, maxiter=1000):\n",
    "    from cvxpy import Variable,Parameter,Minimize,square,norm,Problem\n",
    "    M, N = A.shape\n",
    "    x = np.zeros(N)\n",
    "\n",
    "    AtA = A.T.dot(A)\n",
    "    Atb = A.T.dot(b)\n",
    "\n",
    "    stop_count = 0\n",
    "\n",
    "    # projection helper\n",
    "    x_ = Variable(N)\n",
    "    v_ = Parameter(N)\n",
    "    objective_ =  Minimize(0.5 * norm(x_ - v_, 1))\n",
    "    constraints_ = [sum(x_) == 1]\n",
    "    problem_ = Problem(objective_, constraints_)\n",
    "\n",
    "    def gradient(x):\n",
    "        return AtA.dot(x) - Atb\n",
    "\n",
    "    def obj(x):\n",
    "        return 0.5 * np.linalg.norm(A.dot(x) - b,ord=1)\n",
    "\n",
    "    it = 0\n",
    "    while True:\n",
    "        grad = gradient(x)\n",
    "\n",
    "        # line search\n",
    "        alpha = 1\n",
    "        beta = 0.5\n",
    "        sigma=1e-2\n",
    "        old_obj = obj(x)\n",
    "        while True:\n",
    "            new_x = x - alpha * grad\n",
    "            new_obj = obj(new_x)\n",
    "            if old_obj - new_obj >= sigma * grad.dot(x - new_x):\n",
    "                break\n",
    "            else:\n",
    "                alpha *= beta\n",
    "\n",
    "        x_old = x[:]\n",
    "        x = x - alpha*grad\n",
    "\n",
    "        # projection\n",
    "        v_.value = x\n",
    "        problem_.solve()\n",
    "        x = np.array(x_.value.flat)\n",
    "\n",
    "        y = x + momentum * (x - x_old)\n",
    "\n",
    "        if np.abs(old_obj - obj(x)) < 1e-2:\n",
    "            stop_count += 1\n",
    "        else:\n",
    "            stop_count = 0\n",
    "\n",
    "        if stop_count == 3:\n",
    "            print('early-stopping @ it: ', it)\n",
    "            return x\n",
    "\n",
    "        it += 1\n",
    "\n",
    "        if it == maxiter:\n",
    "            return x\n",
    "\n",
    "x = solve_pg(level0_trainFeatures_rougher, train_df[tgt].values)\n",
    "print('sum x: ', np.sum(x))\n",
    "print(mase(level0_trainFeatures_rougher.dot(x),train_df[tgt].values))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Repeat the procedure with Final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "year = 2019\n",
    "tgt = 'final.output.recovery'\n",
    "\n",
    "X = data_dict[year]['X_train_tsclean']\n",
    "y = data_dict[year]['y_train_tsclean']\n",
    "X_test = data_dict[year]['X_test_ts']\n",
    "\n",
    "mask = data_dict[year]['mask']\n",
    "exclude_pts = data_dict[year]['excl'].set_index('date').tz_localize('UTC')\n",
    "#mask_na_two_row=y[y[tgt].isna()].index.union(y[y[tgt].isna()].index + pd.Timedelta('1 hour')).union(y[y[tgt].isna()].index + pd.Timedelta('2 hour'))\n",
    "inds = mask.index.difference(exclude_pts.index)\n",
    "print(X.shape)\n",
    "\n",
    "X=X.loc[inds,:]\n",
    "y=y.loc[inds,:]\n",
    "mask=mask[inds]\n",
    "print(X.shape)\n",
    "\n",
    "print(f'1) X shape: {X.shape},y: {y.shape}')\n",
    "X = X[mask]\n",
    "y = y[mask][tgt]\n",
    "\n",
    "train_df = pd.concat([X,y],axis= 1)\n",
    "print(f'1) X shape: {X.shape},y: {y.shape}')\n",
    "\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create dicts of stackers and base models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A dictionary of base models\n",
    "scaler = make_pipeline(QuantileTransformer(output_distribution='normal'),PCA(whiten=True))\n",
    "level0_models = {}\n",
    "# level0_models['Lasso'] = make_pipeline(scaler, Lasso(alpha =0.005, random_state=1,max_iter = 2000))\n",
    "# level0_models['ElasticNet'] = make_pipeline(scaler,ElasticNet(alpha = 0.001))\n",
    "def fair_obj(preds, dtrain):\n",
    "    \"\"\"y = c * abs(x) - c**2 * np.log(abs(x)/c + 1)\"\"\"\n",
    "    x = preds - dtrain.get_labels()\n",
    "    c = 1\n",
    "    den = abs(x) + c\n",
    "    grad = c*x / den\n",
    "    hess = c*c / den ** 2\n",
    "    return grad, hess\n",
    "\n",
    "# level0_models['XGB_final_base_a'] = xgb.XGBRegressor(learning_rate=0.05,\n",
    "#                             n_estimators=400,**{'max_depth': 3, 'gamma': '17.158', 'colsample_bytree': '0.442', 'subsample': '0.644', 'min_child_weight': '9.733'},\n",
    "#                             silent=1,\n",
    "#                              random_state =48, nthread = -1)\n",
    "\n",
    "# level0_models['XGB_final_base_b'] =xgb.XGBRegressor(learning_rate=0.05,\n",
    "#                             n_estimators=400,**{'max_depth': 4, 'gamma': '18.571', 'colsample_bytree': '0.745', 'subsample': '0.681', 'min_child_weight': '9.024'},\n",
    "#                             silent=1,\n",
    "#                              random_state =38, nthread = -1)\n",
    "\n",
    "# level0_models['XGB_final_base_c'] = xgb.XGBRegressor(learning_rate=0.05,\n",
    "#                             n_estimators=400,**{'max_depth': 2, 'gamma': '16.491', 'colsample_bytree': '0.522', 'subsample': '0.844', 'min_child_weight': '5.096'},\n",
    "#                             silent=1,\n",
    "#                              random_state =82, nthread = -1)\n",
    "\n",
    "# level0_models['XGB_final_base_d'] = xgb.XGBRegressor(learning_rate=0.05,\n",
    "#                             n_estimators=400,**{'max_depth': 3, 'gamma': '16.766', 'colsample_bytree': '0.540', 'subsample': '0.774', 'min_child_weight': '12.674'},\n",
    "#                             silent=1,\n",
    "#                              random_state =11, nthread = -1)\n",
    "\n",
    "level0_models['LGBM_final_base_a'] = lgb.LGBMRegressor(objective='mae',\n",
    "                              learning_rate=0.05, n_estimators=400,random_state=96,\n",
    "                              **{'max_depth': 6, 'num_leaves': 10, 'feature_fraction': '0.411', 'bagging_fraction': '0.827'})\n",
    "\n",
    "level0_models['LGBM_final_base_b'] =lgb.LGBMRegressor(objective='mae',\n",
    "                              learning_rate=0.05, n_estimators=400,random_state=973,\n",
    "                              **{'max_depth': 6, 'num_leaves': 15, 'feature_fraction': '0.515', 'bagging_fraction': '0.469'})\n",
    "\n",
    "level0_models['LGBM_final_base_c'] =lgb.LGBMRegressor(objective='mae',\n",
    "                              learning_rate=0.05, n_estimators=400,random_state=937,\n",
    "                              **{'max_depth': 3, 'num_leaves': 195, 'feature_fraction': '0.635', 'bagging_fraction': '0.673'})\n",
    "level0_models['LGBM_final_base_d'] =lgb.LGBMRegressor(objective='mae',\n",
    "                              learning_rate=0.05, n_estimators=400,random_state=49,\n",
    "                              **{'max_depth': 4, 'num_leaves': 70, 'feature_fraction': '0.795', 'bagging_fraction': '0.656'})\n",
    "\n",
    "\n",
    "\n",
    "# level0_models['LGBM_final_base_e'] = lgb.LGBMRegressor(objective=obj,\n",
    "#                               learning_rate=0.07, n_estimators=500,random_state=7,\n",
    "#                               **{'max_depth': 4, 'num_leaves': 63, 'feature_fraction': '0.89', 'bagging_fraction': '0.757'})\n",
    "# level0_models['LGBM_final_base_f'] = lgb.LGBMRegressor(objective=obj,\n",
    "#                               learning_rate=0.07, n_estimators=500,random_state=8,\n",
    "#                               **{'max_depth': 4, 'num_leaves': 63, 'feature_fraction': '0.879', 'bagging_fraction': '0.727'})\n",
    "# level0_models['LGBM_final_base_g'] = lgb.LGBMRegressor(objective=obj,\n",
    "#                               learning_rate=0.07, n_estimators=500,random_state=9,\n",
    "#                               **{'max_depth': 5, 'num_leaves': 65, 'feature_fraction': '0.879', 'bagging_fraction': '0.727'})\n",
    "# level0_models['LGBM_final_base_h'] = lgb.LGBMRegressor(objective=obj,\n",
    "#                               learning_rate=0.07, n_estimators=500,random_state=10,\n",
    "#                               **{'max_depth': 4, 'num_leaves': 60, 'feature_fraction': '0.797', 'bagging_fraction': '0.982'})\n",
    "# level0_models['LGBM_final_base_i'] = lgb.LGBMRegressor(objective=obj,\n",
    "#                               learning_rate=0.07, n_estimators=500,random_state=12,\n",
    "#                               **{'max_depth': 5, 'num_leaves': 60, 'feature_fraction': '0.8', 'bagging_fraction': '0.92'})\n",
    "\n",
    "\n",
    "\n",
    "# #level0_models['KNN_final_a'] = make_pipeline(scaler,KNeighborsRegressor(n_jobs = -1,**{'n_neighbors': 254, 'weights': 'distance', 'leaf_size': 16}))\n",
    "# level0_models['KNN_final_b'] = make_pipeline(scaler,KNeighborsRegressor(n_jobs = -1,**{'n_neighbors': 50, 'weights': 'distance', 'leaf_size': 18}))\n",
    "# level0_models['KNN_final_c'] = make_pipeline(scaler,KNeighborsRegressor(n_jobs = -1,**{'n_neighbors': 15, 'weights': 'distance', 'leaf_size': 30.0}))\n",
    "# level0_models['KNN_final_d'] = make_pipeline(scaler,KNeighborsRegressor(n_jobs = -1,**{'n_neighbors': 5, 'weights': 'uniform', 'leaf_size': 24.0}))\n",
    "# level0_models['KNN_rougher_b_bray'] = make_pipeline(scaler,KNeighborsRegressor(n_jobs = -1,**{'n_neighbors': 50, 'weights': 'distance','metric':'braycurtis', 'leaf_size': 18}))\n",
    "# level0_models['KNN_rougher_c_bray'] = make_pipeline(scaler,KNeighborsRegressor(n_jobs = -1,**{'n_neighbors': 15, 'weights': 'distance', 'leaf_size': 30.0,'metric':'braycurtis'}))\n",
    "# level0_models['KNN_rougher_d_bray'] = make_pipeline(scaler,KNeighborsRegressor(n_jobs = -1,**{'n_neighbors': 5, 'weights': 'uniform', 'leaf_size': 24.0,'metric':'braycurtis'}))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train all the base models in the dictionary\n",
    "level0_trainFeatures_final, level0_testFeatures_final = stacks(level0_models, kf, X.columns, tgt, train_df, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A dictionary of level 1 model to train on base model predictions\n",
    "stacker = {\n",
    "           'Enet': ElasticNet(alpha = 0.001),\n",
    "          #'RF':RandomForestRegressor(max_depth = 3,n_estimators=200),\n",
    "          'BR':BayesianRidge(n_iter = int(3e3)),\n",
    "          'Ridge1':Ridge(),\n",
    "          'Ridge1e2':Ridge(alpha=1e2),\n",
    "          'Ridge5e2':Ridge(alpha=5e2),\n",
    "          'Ridge1e3':Ridge(alpha=1e3),\n",
    "          'Ridge5e3':Ridge(alpha=5e3),\n",
    "          'Lasso': Lasso(alpha =0.005, random_state=1,max_iter = 2000),\n",
    "          'LGBM': lgb.LGBMRegressor(objective='mae',\n",
    "                              learning_rate=0.07, n_estimators=500,random_state=40,\n",
    "                              **{'max_depth': 3, 'num_leaves':8, 'feature_fraction': '0.9', 'bagging_fraction': '0.7'})}\n",
    "\n",
    "stackerTraining(stacker, kf, level0_trainFeatures_final, level0_testFeatures_final, train_df,target = tgt)\n",
    "level1_model = stacker['LGBM'].fit(level0_trainFeatures_final,train_df[tgt])\n",
    "level1_test_pred_final = level1_model.predict(level0_testFeatures_final)\n",
    "level1_test_pred_final.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make a submission:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = pd.DataFrame(data = {'date':X_test.index,'rougher.output.recovery':level1_test_pred_rougher, 'final.output.recovery':level1_test_pred_final}).set_index('date')\n",
    "\n",
    "stacked_preds_sub = preds\n",
    "stacked_preds_sub = stacked_preds_sub.reset_index()\n",
    "stacked_preds_sub['date'] = stacked_preds_sub['date'].dt.strftime('%Y-%m-%dT%H:%M:%SZ')\n",
    "stacked_preds_sub.set_index('date',inplace=True)\n",
    "stacked_preds_sub.drop_duplicates(inplace=True)\n",
    "stacked_preds_sub.to_csv('../results/stacked_sub_lgb_lasso_base_alldata_tsclean_fixed_lgbm_lagdiff.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds.plot(style=['o','o'],figsize = (20,10),alpha=0.9)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_av = pd.DataFrame(data = {'date':X_test.index,'rougher.output.recovery':level0_testFeatures_rougher.mean(axis=1), 'final.output.recovery':level0_testFeatures_final.mean(axis=1)})\n",
    "\n",
    "preds_av['date'] = preds_av['date'].dt.strftime('%Y-%m-%dT%H:%M:%SZ')\n",
    "preds_av.set_index('date',inplace=True)\n",
    "preds_av.plot(figsize = (20,10),style=['o','o'],alpha=0.9)\n",
    "\n",
    "preds_av.to_csv('../results/stacked_sub_lgb_lasso_base_alldata_tsclean_r-filt-f-meta.fixed_best_mod_averaged_lagdiff.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = np.power(level0_testFeatures_rougher.prod(axis=1),1/level0_testFeatures_rougher.shape[1])\n",
    "f = np.power(level0_testFeatures_final.prod(axis=1),1/level0_testFeatures_final.shape[1])\n",
    "\n",
    "preds_av = pd.DataFrame(data = {'date':X_test.index,'rougher.output.recovery':r, 'final.output.recovery':f})\n",
    "preds_av['date'] = preds_av['date'].dt.strftime('%Y-%m-%dT%H:%M:%SZ')\n",
    "preds_av.set_index('date',inplace=True)\n",
    "preds_av.plot(figsize = (20,10),style=['o','o'],alpha=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
